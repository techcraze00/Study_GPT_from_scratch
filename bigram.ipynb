{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc09815",
   "metadata": {},
   "source": [
    "Commented cell are meant to explain the code and are not executed. You can execute them to understand better the code.\n",
    "\n",
    "Such as the following code explains how to create input-target pairs for training a language model using a sliding window approach.\n",
    "```python\n",
    "x = train_data[:BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1]\n",
    "\n",
    "for t in range(BLOCK_SIZE):\n",
    "    content = x[:t+1]\n",
    "    target = y[t]\n",
    "    print( f\"When input is {content} the target is {target}\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available()) # Should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe9593",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = './data/input/tinyStoriesData.txt'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # how many independent sequences will we process in parallel?\n",
    "BLOCK_SIZE = 8 # what is the maximum context length for predictions?\n",
    "MAX_ITERS = 3000\n",
    "EVAL_INTERVAL = 300\n",
    "LEARNING_RATE = 1e-2\n",
    "EVAL_ITERS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6f53a-b522-46ff-b8d8-172b0dbc4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv()\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceac54",
   "metadata": {},
   "source": [
    "### TinyStories data loading from HF and saving locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eb63f-85b5-4d18-bcbb-3b1de8c12acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from tqdm.auto import tqdm\n",
    "# ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "\n",
    "# print(ds[\"train\"][\"text\"])\n",
    "\n",
    "# with open(INPUT_DATA_FILE, 'w', encoding='utf-8') as f:\n",
    "#         for i, entry in enumerate(ds):\n",
    "#             # if i >= num_stories:\n",
    "#             #     break\n",
    "#             story_text = entry['text'].strip()\n",
    "            \n",
    "#             f.write(story_text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eaceeb",
   "metadata": {},
   "source": [
    "### Reading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ebb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301edcba",
   "metadata": {},
   "source": [
    "#### get vocab info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f6af2",
   "metadata": {},
   "source": [
    "### Char level encoder-decoder model for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf226b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode(\"prayas\"))\n",
    "print(decode(encode(\"prayas\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1f7f6",
   "metadata": {},
   "source": [
    "### Create data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on a 70% subset of the data for faster training.\n",
    "data_divider = int(0.7 * len(text))\n",
    "data = torch.tensor(encode(text[:data_divider]), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# Split this data into 90% training and 10% validation\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "val_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK_SIZE = 8\n",
    "# train_data[:BLOCK_SIZE+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_data[:BLOCK_SIZE]\n",
    "# y = train_data[1:BLOCK_SIZE+1]\n",
    "\n",
    "# for t in range(BLOCK_SIZE):\n",
    "#     content = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print( f\"When input is {content} the target is {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43761e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# xb, yb = get_batch('train')\n",
    "# print('<------inputs------>')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('\\n<------targets------>')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "\n",
    "# print('-'*20)\n",
    "\n",
    "# for b in range(BATCH_SIZE):\n",
    "#     for t in range(BLOCK_SIZE):\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"When input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e4e9c",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    " \n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()  \n",
    "        # The embedding table consists of rows and columns. for every digits in tensor a row is plucked from this table\n",
    "        # for that particular index and returned. (e.g. in a tensor [21] the 21st row in embedding table is returned)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets= None):\n",
    "        # targets and idx are (B,T) tensor of integer\n",
    "        logits = self.token_embedding_table(idx) # returns an embeddng row from the embedding table\n",
    "                                                    # this consists of (B, T, C) -> (Batch, Time, Channel)\n",
    "\n",
    "        # if target is None then there is no loss to calculate\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            return logits, loss\n",
    "\n",
    "        # we de-construct the values because currently our logits are of shape (BTC)\n",
    "        # and the cross-entrophy fucntion of torch take (BCT) as input\n",
    "        # Pytorch Cross-Entrophy excepts 'C' at 2nd place\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Bsically the function takes (B, T) index and generates (B, T+1), (B, T+2), (B, T+3), .... upto max_new_tokens\"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss= self(idx) # calling the forward function defined above, but notice that the target param is not mentioned, that's because it is optional.\n",
    "\n",
    "            # focusing only on the last time step\n",
    "            logits = logits[:,-1,:] # Becomes (B,C)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # samples from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append samples to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# m = BigramLanguageModel(vocab_size)\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(DEVICE)\n",
    "\n",
    "# logits, loss = m(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "# we are expecting the loss to be -logn(1/VOCAB_SIZE) , we might get something near\n",
    "\n",
    "\n",
    "# sample run for prediction\n",
    "# idx = torch.zeros((1,1), dtype=torch.long, device=DEVICE)\n",
    "# print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits , loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f4ca1",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d672d",
   "metadata": {},
   "source": [
    "### Creating PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=32\n",
    "\n",
    "# for steps in range(10000):\n",
    "#     # sampling a batch of data\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     # evaluate the loss\n",
    "#     logits, loss = m(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=None)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(loss.item())\n",
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long, device=DEVICE), max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GptFromScratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
