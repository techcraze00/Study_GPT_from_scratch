{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4321617",
   "metadata": {},
   "source": [
    "<h4>Change List:</h4>\n",
    "\n",
    "1. Deleting `Head` and `Multi-Head Attention` and writing everything in a single class `CasualSelfAttention`: that handles the projection, splitting into heads, RoPE, and attention all in one place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Tuple    \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "print(\"mps : \",torch.backends.mps.is_available()) # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60663d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = r'/Users/kunal/My Works/Learning/GptFromScratch/data/input/tinyStoriesData.txt'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4 # how many independent sequences will we process in parallel? \n",
    "BLOCK_SIZE = 8 # what is the maximum context length for predictions? ---> 256 to predict all range of tokens from 0-255\n",
    "MAX_ITERS = 1000\n",
    "EVAL_INTERVAL = 300\n",
    "LEARNING_RATE = 3e-4\n",
    "EVAL_ITERS = 200\n",
    "\n",
    "# --------------\n",
    "N_EMBD = 24\n",
    "NUM_HEAD = 2\n",
    "# ----Ever Head is of dim = n_embd//n_head => 486//8 = 64\n",
    "\n",
    "\n",
    "N_LAYER = 6 # Number of transformer block\n",
    "DROPOUT = 0.2 # 20% dropout, Regularizetion to prevent overfitting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3d039",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(f\"Calculated Vocab Size : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8892d5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on a 70% subset of the data for faster training.\n",
    "data_divider = int(0.7 * len(text))\n",
    "data = torch.tensor(encode(text[:data_divider]), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# Split this data into 90% training and 10% validation\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "val_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cc855",
   "metadata": {},
   "source": [
    "## Batch Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b354bf0",
   "metadata": {},
   "source": [
    "## Single Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d201eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs) -> hs: head_size\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # -------------------------\n",
    "        wei = self.dropout(wei)\n",
    "        # -------------------------\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2c93f",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(N_EMBD, N_EMBD)\n",
    "        self.dropout = nn.Dropout(DROPOUT) \n",
    "         \n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cbd583",
   "metadata": {},
   "source": [
    "## RoPE Implementation (Rotary Positional Encoding) from llama 2 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54feef6f",
   "metadata": {},
   "source": [
    "Instead of adding a vector, RoPE rotates the Query and Key vectors.\n",
    "Imagine the Query vector is an arrow on a 2D graph (Channel dim).\n",
    "- If the token is at position 0, we rotate the arrow by 0 degrees.\n",
    "- If the token is at position 1, we rotate the arrow by θ degrees.\n",
    "- If the token is at position 2, we rotate the arrow by 2θ degrees.\n",
    "\n",
    "Because the dot product depends on the angle between vectors, rotating them creates a system where the model cares about the relative distance between tokens, not their absolute position. This allows the model to generalize to longer sequences better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f94d2",
   "metadata": {},
   "source": [
    "### What is freqs_cis ?\n",
    "- freqs: Refers to the \"frequencies\" or angles ($\\theta$) calculated for each dimension of the hidden state.<br>\n",
    "- cis: Refers to the complex exponential form $e^{i\\theta}$, which, according to Euler's formula, is:<br><p>\n",
    "$e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)$\n",
    "\n",
    "`freqs_cis` is a precomputed table (matrix) of complex numbers.\n",
    "It contains the rotation values for every possible position in your sequence (Time) and every pair of dimensions in your head (Head_Dim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    # freqs_cis shape: (seq_len, dim//2)\n",
    "    # x shape: (batch, seq_len, heads, dim//2)\n",
    "    ndim = x.ndim\n",
    "    # Reshape freqs_cis from (seq_len, dim//2) to (1, seq_len, 1, dim//2)\n",
    "    # This allows it to broadcast correctly across batch and heads dimensions\n",
    "    shape = [1, freqs_cis.shape[0], 1, freqs_cis.shape[1]]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # xq shape: (Batch, Time, Heads, Head_Dim)\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Broadcast freqs_cis to match (1, Time, 1, Head_Dim/2)\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    \n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b9dab",
   "metadata": {},
   "source": [
    "Following is a demonstration of how tensor reshaping works in general. This operation will be used in various parts of the transformer implementation, especially in attention mechanisms. hence it's important to understand it well.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import torch  \n",
    "data = torch.tensor([1,2,3,4])  \n",
    "# convert to floating point\n",
    "\n",
    "data.float() # -->  output : tensor([1., 2., 3., 4.])\n",
    "```\n",
    "example 1:\n",
    "```python\n",
    "print(data.float().reshape(*data.shape[:-1],-1,2))\n",
    "```\n",
    "\n",
    "```text\n",
    "<!-- Output -->\n",
    "tensor([[1., 2.],\n",
    "        [3., 4.]])\n",
    "```\n",
    "\n",
    "example 2:\n",
    "```python\n",
    "print(data.float().reshape(*data.shape[:-1],-1,1))\n",
    "```\n",
    "```text\n",
    "<!-- Output -->\n",
    "tensor([[1.],\n",
    "        [2.],\n",
    "        [3.],\n",
    "        [4.]])\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cf1d9",
   "metadata": {},
   "source": [
    "<h3>Reason why Reshaping is important</h3>\n",
    "\n",
    "When using `reshape(*xq.shape[:-1], -1, 2)` <p> The reshape operation changes the shape of `xq` to a new shape defined by the parameters. The `*` operator unpacks the shape tuple, effectively expanding it as separate arguments. The `-1` in reshape is a placeholder that automatically calculates the appropriate size for that dimension based on the total number of elements and the other specified dimensions. The `2` at the end splits the last dimension into two dimensions, which is essential for representing complex numbers (as they consist of real and imaginary parts).\n",
    "\n",
    "For example, if xq initially has a shape (32, 8, 128, 64) (a common shape in transformer models, with 32 being the batch size, 8 the number of heads, 128 the sequence length, and 64 the dimensionality of each head), after this reshaping, it will have the shape (32, 8, 128, 32, 2). The last dimension of 64 is split into two dimensions: 32 for real parts and 2 for representing them as complex numbers (real and imaginary parts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5162aef",
   "metadata": {},
   "source": [
    "<h3>Reason for why we use torch.view_as_complex()</h3> \n",
    "\n",
    "Let's look at an example of what happens when we use `torch.view_as_complex()`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "data = torch.tensor([1,2,3,4])\n",
    "print(torch.view_as_complex(data.float().reshape(*data.shape[:-1],-1,2)))\n",
    "\n",
    "# Output :\n",
    "# tensor([1.+2.j, 3.+4.j])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "<p>\n",
    "\n",
    "Q. **What is is `j` here?** <p>\n",
    "A. In Python, `j` is used to denote the imaginary part of a complex number. So, `1.+2.j` represents the complex number 1 + 2i, where 1 is the real part and 2 is the imaginary part.\n",
    "\n",
    "\n",
    "Q. **Why does this matter?**<p>\n",
    "A. This goes back to the rotation (RoPE). A complex number is just a point on a 2D plane. Rotating a \"pair of numbers\" using standard matrix multiplication is computationally expensive.\n",
    "\n",
    "<u> *Rotating a \"single complex number\" is just a simple multiplication.* </u>\n",
    "\n",
    "By converting the tensor to complex mode (with j), PyTorch can rotate the embeddings much faster.\n",
    "\n",
    "> [!NOTE] \n",
    "> The essence of RoPE is that by applying these phase shifts, the dot product (used in the self-attention mechanism) between queries and keys becomes sensitive to their relative positions. It's not merely about measuring \"real angle distance\" but rather about how the phase-shifted dot product correlates with the positional relationships of tokens in the input sequence.\n",
    "\n",
    "By converting xq and xk into complex tensors, these lines prepare them for such a rotation. The actual rotation is performed later in the code, where these complex tensors (xq_ and xk_) are multiplied by another complex tensor representing the rotation (typically through phase factors, like freqs_cis in this code).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a11830",
   "metadata": {},
   "source": [
    "## Casual Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cc70c",
   "metadata": {},
   "source": [
    "Instead of having separate classes for `Head` and `Multi-Head Attention`, we can consolidate everything into a single class called `CasualSelfAttention`. This class will handle the projection of inputs, splitting into multiple heads, applying RoPE (Rotary Positional Encoding), and performing the attention mechanism all in one place. This approach simplifies the architecture and makes it easier to manage the attention mechanism as a whole.\n",
    "\n",
    "The key changes includes:\n",
    "- **Projection**: The class will include linear layers to project the input embeddings into query, key, and value vectors. Previoulsy we had Q, K and V as three seperate layers inside a loop. But here we do one massive matrx multiplication. For example if your embedding size is 64, we project it to 192 (64 for Query + 64 for Key + 64 for Value). This is significantly faster on GPUs than doing 3 smaller operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd//n_head\n",
    "\n",
    "        # KEY CHANGE 1 in CasualAttention: Calculate Q, K, V in ONE Linear layer\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "\n",
    "        # This is a linear layer applied after attention is calculated. It allows the results from all the different heads to \"talk\" to each other and mix their features before moving to the next block.\n",
    "        # Previously this was named as self.proj\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT) \n",
    "\n",
    "        # We create the lower-triangular mask (ones in the bottom-left, zeros in the top-right). We call it a \"buffer\" because it is a tensor that is part of the model state, but it is not a trainable parameter (gradients won't update it).\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)).view(1, 1, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        \n",
    "    def forward(self, x, freqs_cis = None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Calculate Q, K, V for ALL heads at once\n",
    "        # Result shape: (B, T, 3 * n_embd) --> shape (B, T, 3 * C)\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        # Split Q, K, V\n",
    "        # New shape: (B, T, n_head, head_dim)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, self.head_dim)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim)\n",
    "\n",
    "        # Before we calculate attention, we need to inject \"position\" information. We rotate the q and k vectors based on their position in the sequence (index T).\n",
    "        # Apply RoPE (Rotary Positional Embeddings)\n",
    "        if freqs_cis is not None:\n",
    "            # Slice freqs_cis to match the current sequence length T\n",
    "            freqs_cis_sliced = freqs_cis[:T]\n",
    "            q, k = apply_rotary_emb(q, k, freqs_cis_sliced)\n",
    "\n",
    "        # Transpose for Matrix Multiplication\n",
    "        # PyTorch performs matrix multiplication on the last two dimensions. We want to do attention on the 'Time' dimension for every 'Head' independently.\n",
    "        # i.e. (B, Time, Heads, Dim) → (B, Heads, Time, Dim).\n",
    "        q = q.transpose(1, 2) \n",
    "        k = k.transpose(1, 2) \n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Self Attention Score Calculation (Scaled Dot Product)\n",
    "        # (B, Heads, T, T) @ (B, Heads, T, Dim) -> (B, Heads, T, T)\n",
    "        #  We scale by 1/sqrt(head_dim) to keep the numbers stable.\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))  \n",
    "        \n",
    "        # Causal Masking, coverting zeros to -inf to avoid future token knowledge\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # We calculate the weighted sum of the values based on how \"interesting\" (attention score) they were.\n",
    "        # (B, Heads, T, T) @ (B, Heads, T, Dim) → (B, Heads, T, Dim)\n",
    "        y = att @ v\n",
    "\n",
    "        # Reassemble Heads:\n",
    "        # We are done processing heads separately. We transpose back to (B, T, Heads, Dim) and then view (flatten) the last two dimensions back into (B, T, C).\n",
    "        # we use contiguous is a Pytorch function that moves the data physcially into contiguous memory because 'view' requires the data to be physically contiguous in memory\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "    \n",
    "        # The result of the attention is passed through one final linear layer to mix the information found by different heads.\n",
    "        return self.dropout(self.c_proj(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59a2cb",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46438df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"This is a single linear layer followed by non-linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        As per the paper of GPT2, they implemented the FFN as:\n",
    "        The outer layer was of size 512 and their inner layer was of size 2048 i.e. outer layer = 4 x inner layer\n",
    "\n",
    "        Hence when going inside the FFN we will use the dimension = n_embd and the inner layer will be 4 x n_embd\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd), # ---> this particular layer is the projection layer that goes back in the residual pathway       \n",
    "            # dropout is something that is added right before the residual connection gets back in the residual pathway\n",
    "            nn.Dropout(DROPOUT) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea07b6",
   "metadata": {},
   "source": [
    "## RMS Normalization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1d0b2",
   "metadata": {},
   "source": [
    "Standard Layer Normalization re-centers data around zero (subtracting the mean) and then scales it (dividing by variance). RMSNorm assumes the mean is already close to zero and skips the subtraction. It only scales the input based on the Root Mean Square (RMS).\n",
    "\n",
    "For our problem we will use RMSNorm as :\n",
    "$\\bar{x}_i = \\frac{x_i}{\\text{RMS}(x)} \\cdot g_i$\n",
    "\n",
    "Where : $\\text{RMS}(x) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\epsilon}$\n",
    "\n",
    "----\n",
    "<mark>So basically we calulate the mean of sqared values of the input, take square root. Then take reciprocal of that and multiply with input \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        # eps (epsilon): A tiny number added to prevent division by zero errors during calculation.\n",
    "        self.eps = eps\n",
    "\n",
    "        # weights are initialized as ones beacuase this will not change the magnitude of the output.\n",
    "        # during training it will learn to scale normalized output to the best value for the network.\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def _norm(self, x):\n",
    "        # here mean is calulated for the last dimention i.e. Feature/Channel dimensionx \n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # using float32 (full-precision) for stability during norm calculation\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9befa72",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6944ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"A transformer Block comprising of Attention, FFN and BLM\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "\n",
    "        # self.sa = MultiHeadAttention( n_head, head_size)  \n",
    "        self.sa = CasualHeadAttention(n_embd, n_head)\n",
    "        self.fwd = FeedForward(n_embd)\n",
    "\n",
    "        # using RMSNorm\n",
    "        self.ln1 = nn.RMSNorm(n_embd)\n",
    "        self.ln2 = nn.RMSNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x, freqs_cis):\n",
    "        # In `x + self.sa(x)` or `x + self.fwd(x)` the `x + _` denotes that we fork from x (i.e. residuals are added at the Self-attention step and FFN step) \n",
    "        # Layer normalization is applied immediately on x\n",
    "        # Since nn.Sequential expects layers to take only one input, but CausalSelfAttention now needs x AND freqs_cis. Hence we pass freqs_cis here\n",
    "        x = x + self.sa(self.ln1(x), freqs_cis)\n",
    "        x = x + self.fwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855517f4",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        \n",
    "        # NOTE: We NO LONGER need position_embedding_table because RoPE handles it!\n",
    "        # self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD) \n",
    "        \n",
    "        # PRECOMPUTE FREQUENCIES HERE\n",
    "        head_dim = N_EMBD // NUM_HEAD\n",
    "\n",
    "        # Precompute for the maximum block size\n",
    "        freqs_cis = precompute_freqs_cis(head_dim, BLOCK_SIZE)\n",
    "\n",
    "        # Register as buffer so it saves with model and moves to GPU\n",
    "        self.register_buffer('freqs_cis', freqs_cis)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(N_EMBD, n_head=NUM_HEAD) for _ in range(N_LAYER)\n",
    "        ])\n",
    "        self.ln_f = RMSNorm(N_EMBD) # Final normalization\n",
    "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token embeddings only (No Positional Embeddings added here!)\n",
    "        x = self.token_embedding_table(idx)\n",
    "        \n",
    "        # PASS FREQS_CIS INTO BLOCKS\n",
    "        # We can't use nn.Sequential directly anymore because we need to pass 2 arguments (x, freqs_cis)\n",
    "        # So we loop through blocks manually\n",
    "        for block in self.blocks:\n",
    "            x = block(x, self.freqs_cis) \n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last BLOCK_SIZE tokens\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            # Get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# m = BigramLanguageModel(vocab_size)\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becaa9d0",
   "metadata": {},
   "source": [
    "## Loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5241ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743316ac",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9008be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd7a8a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nTraining loop completed successfully!\")\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Can you share it with me and\"\n",
    "\n",
    "context = torch.zeros(encode([test]), dtype=torch.long, device=DEVICE)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.float().reshape(*data.shape[:-1],-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.view_as_complex(data.float().reshape(*data.shape[:-1],-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9f3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GptFromScratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
